{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled8.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOw/kLse80QtZ5MBzI6gy2o",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/narendra974/insidedeeplearning/blob/main/Seq2SeqTraining.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "8vqMkgwpMV_t"
      },
      "outputs": [],
      "source": [
        "from io import BytesIO\n",
        "from zipfile import ZipFile\n",
        "from urllib.request import urlopen\n",
        "import re\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import time\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import accuracy_score\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from tqdm.autonotebook import tqdm\n",
        "from torchvision import datasets\n",
        "from torchvision.transforms import ToTensor, Lambda"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "all_data = []\n",
        "resp = urlopen(\"https://download.pytorch.org/tutorial/data.zip\")\n",
        "zipfile = ZipFile(BytesIO(resp.read()))\n",
        "for line in zipfile.open('data/eng-fra.txt').readlines():\n",
        "  line = line.decode('utf-8').lower()\n",
        "  line = re.sub(r\"[-.!?]+\", r\" \", line) # remove punctuation.\n",
        "  source_lang, target_lang = line.split('\\t')[0:2]\n",
        "  all_data.append( (source_lang.strip(), target_lang.strip()) )"
      ],
      "metadata": {
        "id": "7e9cc1MJNj3L"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "short_subset = []\n",
        "MAX_LEN = 6\n",
        "for (s, t) in all_data:\n",
        "  if max( len(s.split(\" \")), len(t.split(\" \")) ) <= MAX_LEN:\n",
        "    short_subset.append((s, t))\n",
        "\n",
        "print(\"Using \", len(short_subset), \"/\", len(all_data))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vvP3MzQcjI-3",
        "outputId": "fdc46394-7745-4157-a248-1fca6b7988c6"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using  66251 / 135842\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "SOS_token = '<SOS>'  # Start of Sentence\n",
        "EOS_token = \"<EOS>\"  # End of Sentence\n",
        "PAD_token = \"_PADDING_\"  # Padding Token\n",
        "\n",
        "word2index = {PAD_token:0, SOS_token:1, EOS_token:2}\n",
        "\n",
        "for s, t in short_subset:\n",
        "  for sentence in (s, t):\n",
        "    for word in sentence.split(\" \"):\n",
        "      if word not in word2index:\n",
        "        word2index[word] = len(word2index)\n",
        "print('size of vocab ', len(word2index))\n",
        "\n",
        "indextoword = {}\n",
        "for word, index in word2index.items():\n",
        "  indextoword[index] = word\n",
        "\n"
      ],
      "metadata": {
        "id": "0N-8rjOmnQdl",
        "outputId": "2b83f2c8-6ccb-4e73-f449-97916f37ec60",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "size of vocab  24577\n"
          ]
        }
      ]
    }
  ]
}