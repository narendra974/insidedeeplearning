{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "FC Networks.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyM4JcHCW3pJL+RnDhzccqhO",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/narendra974/insidedeeplearning/blob/main/FC_Networks.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Fully Connected Networks\n",
        "\n",
        "To implement any kind of neural network in PyTorch we must phrase the problem as an optimization problem (function minimization)\n",
        "\n",
        "1. Input Data and labels are feed into the process.\n",
        "2. Output of the model is used with the true label to compute a loss.\n",
        "3. quantifying how bad the model is doing with the loss function.\n",
        "4. Compute the gradient of loss with each parameter. \n",
        "5. Update the parameters with the gradients calculated.\n",
        "\n"
      ],
      "metadata": {
        "id": "UueO1ltINFve"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Function Minimization Problem\n",
        "\n",
        "Alterthe parameters 'Theta' to minimize the error/lossof the neural network's prediction against the correct predictions over the entire dataset."
      ],
      "metadata": {
        "id": "Cg78AD2rQz2v"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Parameter Learning Rule(Gradient Descent)\n",
        "\n",
        "The new parameters Theta(k+1) are equal to the old parameters minus the gradient with respect to the old parameters of the error/loss of the neural networks prediction against the correct predictions averaged over the entire dataset and down-weighted by the learning rate."
      ],
      "metadata": {
        "id": "ONOC6aqyQ-zJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import *\n",
        "from tqdm.autonotebook import tqdm"
      ],
      "metadata": {
        "id": "e7Rr0Kj6R4ZH"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def moveTo(obj, device):\n",
        "  if isinstance(obj, list):\n",
        "    return [moveTo(x, device) for x in obj]\n",
        "  elif isinstance(obj, tuple):\n",
        "    return tuple(moveTo(list(obj), device))\n",
        "  elif isinstance(obj, set):\n",
        "    return set(moveTo(list(obj), device))\n",
        "  elif isinstance(obj, dict):\n",
        "    to_ret = dect()\n",
        "    for key, value in obj.items():\n",
        "      to_ret[moveTo(key, device)] = moveTo(value,device)\n",
        "  elif hasattr(obj,\"to\"):\n",
        "    return obj.to(device)\n",
        "  else:\n",
        "    return object"
      ],
      "metadata": {
        "id": "gpB_cuxgUzII"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_simple_network(model, loss_fun, training_loader, epochs=20, device=\"cpu\"):\n",
        "  optimizer = torch.optim.SGD(model.parameters(), lr=0.001) # Learning rate to be provided.\n",
        "  for epoch in tqdm(range(epochs), desc=\"Epoch\"):\n",
        "\n",
        "    model = model.train()  # put the model to train\n",
        "    running_loss = 0.0\n",
        "\n",
        "    for inputs, labels in tqdm(training_loader, desc='Batch', Leave=False):\n",
        "      inputs = moveTo(inputs, device)\n",
        "      labels = moveTo(labels, device)\n",
        "      optimizer.zero_grad() # remove old information from previous iteration.\n",
        "      predicted_labels = model(inputs)\n",
        "      loss = loss_fun(labels, predicted_labels)\n",
        "      loss.backward() # for computing the gradients. \n",
        "      optimizer.step()   # Update the parameters. \n",
        "      running_loss = running_loss + loss.item()\n",
        " \n"
      ],
      "metadata": {
        "id": "THjXm9eYSqtb"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "gZcjONWGUx7W"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}