{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled7.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPhgvSCJgZMpn4oucl5Esle",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/narendra974/insidedeeplearning/blob/main/AutoRegressieveModel.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "_FHkCw7pNWcN"
      },
      "outputs": [],
      "source": [
        "from io import BytesIO\n",
        "from zipfile import ZipFile\n",
        "from urllib.request import urlopen\n",
        "import re\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import time\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import accuracy_score\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from tqdm.autonotebook import tqdm\n",
        "from torchvision import datasets\n",
        "from torchvision.transforms import ToTensor, Lambda"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
        "\n",
        "def moveTo(obj, device):\n",
        "  if hasattr(obj,\"to\"):\n",
        "    return obj.to(device)\n",
        "  elif isinstance(obj, list):\n",
        "    return [moveTo(x, device) for x in obj]\n",
        "  elif isinstance(obj, tuple):\n",
        "    return tuple(moveTo(list(obj), device))\n",
        "  elif isinstance(obj, set):\n",
        "    return set(moveTo(list(obj), device))\n",
        "  elif isinstance(obj, dict):\n",
        "    to_ret = dect()\n",
        "    for key, value in obj.items():\n",
        "      to_ret[moveTo(key, device)] = moveTo(value,device)\n",
        "    return to_ret\n",
        "  else:\n",
        "    return object\n",
        "\n",
        "\n",
        "def run_epoch(model, optimizer, data_loader, loss_func, device, results, score_funcs, prefix=\"\", desc=None):\n",
        "    \"\"\"\n",
        "    model -- the PyTorch model / \"Module\" to run for one epoch\n",
        "    optimizer -- the object that will update the weights of the network\n",
        "    data_loader -- DataLoader object that returns tuples of (input, label) pairs. \n",
        "    loss_func -- the loss function that takes in two arguments, the model outputs and the labels, and returns a score\n",
        "    device -- the compute lodation to perform training\n",
        "    score_funcs -- a dictionary of scoring functions to use to evalue the performance of the model\n",
        "    prefix -- a string to pre-fix to any scores placed into the _results_ dictionary. \n",
        "    desc -- a description to use for the progress bar.     \n",
        "    \"\"\"\n",
        "    running_loss = []\n",
        "    y_true = []\n",
        "    y_pred = []\n",
        "    start = time.time()\n",
        "    for inputs, labels in tqdm(data_loader, desc=desc, leave=False):\n",
        "        #Move the batch to the device we are using. \n",
        "        inputs = moveTo(inputs, device)\n",
        "        labels = moveTo(labels, device)\n",
        "        y_hat = model(inputs) #this just computed f_Î˜(x(i))\n",
        "        # Compute loss.\n",
        "\n",
        "        loss = loss_func(y_hat, labels)\n",
        "\n",
        "        if model.training:\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "        #Now we are just grabbing some information we would like to have\n",
        "        running_loss.append(loss.item())\n",
        "\n",
        "        if len(score_funcs) > 0 and isinstance(labels, torch.Tensor):\n",
        "            #moving labels & predictions back to CPU for computing / storing predictions\n",
        "            labels = labels.detach().cpu().numpy()\n",
        "            y_hat = y_hat.detach().cpu().numpy()\n",
        "            #add to predictions so far\n",
        "            y_true.extend(labels.tolist())\n",
        "            y_pred.extend(y_hat.tolist())\n",
        "    #end training epoch\n",
        "    end = time.time()\n",
        "    \n",
        "    y_pred = np.asarray(y_pred)\n",
        "    if len(y_pred.shape) == 2 and y_pred.shape[1] > 1: #We have a classification problem, convert to labels\n",
        "        y_pred = np.argmax(y_pred, axis=1)\n",
        "    #Else, we assume we are working on a regression problem\n",
        "    \n",
        "    results[prefix + \" loss\"].append( np.mean(running_loss) )\n",
        "    for name, score_func in score_funcs.items():\n",
        "        try:\n",
        "            results[prefix + \" \" + name].append( score_func(y_true, y_pred) )\n",
        "        except:\n",
        "            results[prefix + \" \" + name].append(float(\"NaN\"))\n",
        "    return end-start #time spent on epoch\n",
        "\n",
        "\n",
        "def train_simple_network(model, loss_func, train_loader, test_loader=None, score_funcs=None, \n",
        "                         epochs=50, device=\"cpu\", checkpoint_file=None, lr=0.001):\n",
        "    \"\"\"Train simple neural networks\n",
        "    \n",
        "    Keyword arguments:\n",
        "    model -- the PyTorch model / \"Module\" to train\n",
        "    loss_func -- the loss function that takes in batch in two arguments, the model outputs and the labels, and returns a score\n",
        "    train_loader -- PyTorch DataLoader object that returns tuples of (input, label) pairs. \n",
        "    test_loader -- Optional PyTorch DataLoader to evaluate on after every epoch\n",
        "    score_funcs -- A dictionary of scoring functions to use to evalue the performance of the model\n",
        "    epochs -- the number of training epochs to perform\n",
        "    device -- the compute lodation to perform training\n",
        "    \n",
        "    \"\"\"\n",
        "    to_track = [\"epoch\", \"total time\", \"train loss\"]\n",
        "    if test_loader is not None:\n",
        "        to_track.append(\"test loss\")\n",
        "    if score_funcs is not None:\n",
        "      for eval_score in score_funcs:\n",
        "        to_track.append(\"train \" + eval_score )\n",
        "        if test_loader is not None:\n",
        "            to_track.append(\"test \" + eval_score )\n",
        "        \n",
        "    total_train_time = 0 #How long have we spent in the training loop? \n",
        "    results = {}\n",
        "    #Initialize every item with an empty list\n",
        "    for item in to_track:\n",
        "        results[item] = []\n",
        "        \n",
        "    #SGD is Stochastic Gradient Decent.\n",
        "    optimizer = torch.optim.SGD(model.parameters(), lr=0.001)\n",
        "    #Place the model on the correct compute resource (CPU or GPU)\n",
        "    model.to(device)\n",
        "    for epoch in tqdm(range(epochs), desc=\"Epoch\"):\n",
        "        model = model.train()#Put our model in training mode\n",
        "        \n",
        "        total_train_time += run_epoch(model, optimizer, train_loader, loss_func, device, results, score_funcs, prefix=\"train\", desc=\"Training\")\n",
        "\n",
        "        results[\"total time\"].append( total_train_time )\n",
        "        results[\"epoch\"].append( epoch )\n",
        "        \n",
        "        if test_loader is not None:\n",
        "            model = model.eval()\n",
        "            with torch.no_grad():\n",
        "                run_epoch(model, optimizer, test_loader, loss_func, device, results, score_funcs, prefix=\"test\", desc=\"Testing\")\n",
        "                    \n",
        "    if checkpoint_file is not None:\n",
        "        torch.save({\n",
        "            'epoch': epoch,\n",
        "            'model_state_dict': model.state_dict(),\n",
        "            'optimizer_state_dict': optimizer.state_dict(),\n",
        "            'results' : results\n",
        "            }, checkpoint_file)\n",
        "\n",
        "    return pd.DataFrame.from_dict(results)\n",
        "\n",
        "\n",
        "def train_network(model, loss_func, train_loader, val_loader=None, test_loader=None,score_funcs=None, \n",
        "                         epochs=50, device=\"cpu\", checkpoint_file=None, \n",
        "                         lr_schedule=None, optimizer=None, disable_tqdm=False\n",
        "                        ):\n",
        "    \"\"\"Train simple neural networks\n",
        "    \n",
        "    Keyword arguments:\n",
        "    model -- the PyTorch model / \"Module\" to train\n",
        "    loss_func -- the loss function that takes in batch in two arguments, the model outputs and the labels, and returns a score\n",
        "    train_loader -- PyTorch DataLoader object that returns tuples of (input, label) pairs. \n",
        "    val_loader -- Optional PyTorch DataLoader to evaluate on after every epoch\n",
        "    test_loader -- Optional PyTorch DataLoader to evaluate on after every epoch\n",
        "    score_funcs -- A dictionary of scoring functions to use to evalue the performance of the model\n",
        "    epochs -- the number of training epochs to perform\n",
        "    device -- the compute lodation to perform training\n",
        "    lr_schedule -- the learning rate schedule used to alter \\eta as the model trains. If this is not None than the user must also provide the optimizer to use. \n",
        "    optimizer -- the method used to alter the gradients for learning. \n",
        "    \n",
        "    \"\"\"\n",
        "    if score_funcs == None:\n",
        "        score_funcs = {}#Empty set \n",
        "    \n",
        "    to_track = [\"epoch\", \"total time\", \"train loss\"]\n",
        "    if val_loader is not None:\n",
        "        to_track.append(\"val loss\")\n",
        "    if test_loader is not None:\n",
        "        to_track.append(\"test loss\")\n",
        "    for eval_score in score_funcs:\n",
        "        to_track.append(\"train \" + eval_score )\n",
        "        if val_loader is not None:\n",
        "            to_track.append(\"val \" + eval_score )\n",
        "        if test_loader is not None:\n",
        "            to_track.append(\"test \"+ eval_score )\n",
        "        \n",
        "    total_train_time = 0 #How long have we spent in the training loop? \n",
        "    results = {}\n",
        "    #Initialize every item with an empty list\n",
        "    for item in to_track:\n",
        "        results[item] = []\n",
        "\n",
        "        \n",
        "    if optimizer == None:\n",
        "        #The AdamW optimizer is a good default optimizer\n",
        "        optimizer = torch.optim.AdamW(model.parameters())\n",
        "        del_opt = True\n",
        "    else:\n",
        "        del_opt = False\n",
        "\n",
        "    #Place the model on the correct compute resource (CPU or GPU)\n",
        "    model.to(device)\n",
        "    for epoch in tqdm(range(epochs), desc=\"Epoch\", disable=disable_tqdm):\n",
        "        model = model.train()#Put our model in training mode\n",
        "\n",
        "        total_train_time += run_epoch(model, optimizer, train_loader, loss_func, device, results, score_funcs, prefix=\"train\", desc=\"Training\")\n",
        "        \n",
        "        results[\"epoch\"].append( epoch )\n",
        "        results[\"total time\"].append( total_train_time )\n",
        "        \n",
        "      \n",
        "        if val_loader is not None:\n",
        "            model = model.eval() #Set the model to \"evaluation\" mode, b/c we don't want to make any updates!\n",
        "            with torch.no_grad():\n",
        "                run_epoch(model, optimizer, val_loader, loss_func, device, results, score_funcs, prefix=\"val\", desc=\"Validating\")\n",
        "                \n",
        "        #In PyTorch, the convention is to update the learning rate after every epoch\n",
        "        if lr_schedule is not None:\n",
        "            if isinstance(lr_schedule, torch.optim.lr_scheduler.ReduceLROnPlateau):\n",
        "                lr_schedule.step(results[\"val loss\"][-1])\n",
        "            else:\n",
        "                lr_schedule.step()\n",
        "                \n",
        "        if test_loader is not None:\n",
        "            model = model.eval() #Set the model to \"evaluation\" mode, b/c we don't want to make any updates!\n",
        "            with torch.no_grad():\n",
        "                run_epoch(model, optimizer, test_loader, loss_func, device, results, score_funcs, prefix=\"test\", desc=\"Testing\")\n",
        "        \n",
        "        \n",
        "        if checkpoint_file is not None:\n",
        "            torch.save({\n",
        "                'epoch': epoch,\n",
        "                'model_state_dict': model.state_dict(),\n",
        "                'optimizer_state_dict': optimizer.state_dict(),\n",
        "                'results' : results\n",
        "                }, checkpoint_file)\n",
        "    if del_opt:\n",
        "        del optimizer\n",
        "\n",
        "    return pd.DataFrame.from_dict(results)\n",
        "\n",
        "\n",
        "def prediction(model, img):\n",
        "  with torch.no_grad():\n",
        "    w, h = img.shape\n",
        "    if not isinstance(img, torch.Tensor):\n",
        "      img = torch.tensor(img)\n",
        "    x = img.reshape(1, -1, w, h)\n",
        "    logits = model(x)\n",
        "    y_hat = F.softmax(logits, dim=1)\n",
        "    return y_hat.numpy().flatten()"
      ],
      "metadata": {
        "id": "_gaPxme4LgKV"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "printonce = True\n",
        "all_data = []\n",
        "resp = urlopen(\"https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt\")\n",
        "shakespear_100k = resp.read()\n",
        "shakespear_100k = shakespear_100k.decode('utf-8').lower()\n",
        "device = torch.device(\"cuda\")\n"
      ],
      "metadata": {
        "id": "XjoQlhCfNwU9"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocab2index = {}\n",
        "\n",
        "for char in shakespear_100k:\n",
        "  if char not in vocab2index:\n",
        "    vocab2index[char] = len(vocab2index)\n",
        "\n",
        "index2Vocab={}\n",
        "\n",
        "for k, v in vocab2index.items():\n",
        "  index2Vocab[v]=k\n",
        "\n",
        "\n",
        "print(\"Vocab Size \", len(vocab2index))\n",
        "print(\"Total text size \", len(shakespear_100k))\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CYb4ehGNREZz",
        "outputId": "33115b7d-fd1b-4675-a806-09f28e0e3fc3"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocab Size  39\n",
            "Total text size  1115394\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class AutoRegressiveDataset(Dataset):\n",
        "\n",
        "  def __init__(self, large_string, max_chunk=500):\n",
        "    self.doc = large_string\n",
        "    self.max_chunk = max_chunk\n",
        "\n",
        "  def __len__(self):\n",
        "    return (len(self.doc)-1) // self.max_chunk\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    start = idx * self.max_chunk\n",
        "    sub_string = self.doc[start:start+self.max_chunk]\n",
        "    x = [vocab2index(c) for c in sub_string]\n",
        "    sub_string_y = self.doc[start+1:start+self.max_chunk+1]\n",
        "    y = [vocab2index(c) for c in sub_string_y]\n",
        "    "
      ],
      "metadata": {
        "id": "aHSqHvh_dKEm"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class AutoRegressiveModule(nn.Module):\n",
        "  \n",
        "  def __init__(self, num_embeddings, embd_size, hidden_size, layers=1):\n",
        "    super(AutoRegressiveModule, self).__init__()\n",
        "    self.hidden_size = hidden_size\n",
        "    self.embd = nn.Embedding(num_embeddings, embd_size)\n",
        "    self.layers = nn.ModuleList( [nn.GRUCell(embd_size, hidden_size)] + [nn.GRUCell(hidden_size, hidden_size) for i in range(layers-1)])\n",
        "    self.norms = nn.ModuleList([nn.LayerNorm(hidden_size) for i in range(layers)])\n",
        "    self.pred_class = nn.Sequential(\n",
        "        nn.Linear(hidden_size, hidden_size), \n",
        "        nn.LeakyReLU(),\n",
        "        nn.LayerNorm(hidden_size),\n",
        "        nn.Linear(hidden_size, num_embeddings)   \n",
        "    )\n",
        "\n",
        "\n",
        "  def initHiddenStates(self, B):\n",
        "    return [torch.zeros(B, self.hidden_size, device=device) for _ in range(len(self.layers))]\n",
        "\n",
        "\n",
        "  def step(self, x_in, h_prevs):\n",
        "    if len(x_in.shape) == 1:\n",
        "      x_in = self.embd(x_in)\n",
        "  \n",
        "    if h_prevs is None:\n",
        "      h_prevs = self.initHiddenStates(x_in.shape[0])\n",
        "\n",
        "    # for GRUCell there is only one input that is send as the hidden state to the next time step process and as an output from this layer. \n",
        "    for l in range(len(self.layers)):  \n",
        "      h_prev = h_prevs[l]\n",
        "      h = self.norms[l](self.layers[l](x_in, h_prev))\n",
        "      h_prevs[l]=h\n",
        "      x_in = h\n",
        "\n",
        "    return self.pred_class(x_in)\n",
        "\n",
        "\n",
        "  def forward(self, input):\n",
        "    B = input.size(0)\n",
        "    T = input.size(1)\n",
        "    x = self.embd(input)\n",
        "    h_prevs = self.initHiddenStates(B)\n",
        "    \n",
        "    last_activations = []\n",
        "    for t in range(T):\n",
        "      x_in = x[:,t, :]\n",
        "      last_activations.append(self.step(x_in, h_prevs))\n",
        "    \n",
        "    last_activations = torch.stack(last_activations, dim=1)\n",
        "    return last_activations"
      ],
      "metadata": {
        "id": "n23Lv2xWtc7I"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def crossEntLossTime(x, y):\n",
        "\n",
        "  cel = nn.CrossEntropyLoss()\n",
        "  T = x.size(1)\n",
        "  loss = 0  \n",
        "\n",
        "  if printonce is True:\n",
        "    print(x.shape)\n",
        "    print(y.shape)\n",
        "    printonce=False\n",
        "\n",
        "  for t in range(T):\n",
        "    loss += cel(x[:,t,:], y[:, t])\n",
        "\n",
        "  return loss\n"
      ],
      "metadata": {
        "id": "IFB6LeR2APNg"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "autoRegData = AutoRegressiveDataset(shakespear_100k, max_chunk=250)\n",
        "autoRegLoader = DataLoader(autoRegData, batch_size=128, shuffle=True)\n",
        "\n",
        "autoReg_model = AutoRegressiveModule(len(vocab2index), 32, 128, layers=2)\n",
        "autoReg_model = autoReg_model.to(device)\n",
        "\n",
        "\n",
        "for p in autoReg_model.parameters():\n",
        "  p.register_hook(lambda grad: torch.clamp(grad, -2, 2))"
      ],
      "metadata": {
        "id": "-OxIsGbCPqPl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.nn.modules.loss import CrossEntropyLoss\n",
        "train_network(autoReg_model, crossEntLossTime, autoRegLoader, epochs=100, device=device)"
      ],
      "metadata": {
        "id": "PZJ28mkZSVJT"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}